#!/bin/sh
#
# Batch submission script for 5,526 dask workers on summit.  This
# presumes six workers per Summit node, which is a common configuration.  Feel
# free to copy and hack to fit.
#
# Mark Coletti, colettima@ornl.gov
#
#BSUB -P CSC363
#BSUB -W 2:00
#BSUB -nnodes 921
#BSUB -J 921_node_dask
#BSUB -o out.dask.%J
#BSUB -e err.dask.%J

export PROJ_DIR=/ccs/proj/csc363/
export SRC_PATH=${PROJ_DIR}/may/
export RUN_DIR=/gpfs/alpine/csc363/scratch/mcoletti/921_node_scaling_test
export SCHEDULER_FILE=${RUN_DIR}/scheduler_file.json


# Count the unique host names and subtract the batch node to get
# the actual number of nodes allocated to this job.
let num_nodes=(`cat $LSB_DJOB_HOSTFILE | sort | uniq | wc -l` - 1)

# We want six workers per node
let num_workers=($num_nodes * 6)

export NUM_WORKERS=$num_workers

# To shut up terminal warning
export LC_ALL=C
export LANG=C

module purge

# Just a sanity check to ensure we have the correct modules loaded
module list

# Presumes that "make venv" has already been done in the LEAP top level to
# create the python virtual environment.
source  /ccs/proj/YOURPROJ/path/to/your/conda/or/venv/activate

# Just a sanity check to ensure we're using the *correct* python environment.
echo "Using python: " `which python3`
echo "PYTHONPATH: " $PYTHONPATH

# Just in case I forgot to make the durn thing ahead of time.  :P
mkdir -p $RUN_DIR

cd $RUN_DIR

# Copy over the hosts allocated for this job so that we can later verify
# that all the allocated nodes were busy with the correct worker allocation.
cp $LSB_DJOB_HOSTFILE $LSB_JOBID.hosts

# The scheduler doesn't need GPUs. It just needs one lonely core to run on.
jsrun --gpu_per_rs 0 --nrs 1 --tasks_per_rs 1 --cpu_per_rs 1 --rs_per_host 1 dask-scheduler --interface ib0 --no-dashboard --no-show --scheduler-file $SCHEDULER_FILE &

# Give the scheduler a chance to spin up.
sleep 10

echo Starting workers

# Fan out the dask workers to their resource sets.  Note that each worker will
# write out to separate stdout/stderr files, which will generate copious files,
# and which will make it a heckuvalot easier to find that one worker output.
# We allocate a single GPU and CPU per worker.
jsrun --smpiargs="none" --nrs $NUM_WORKERS -e individual --stdio_stdout ${RUN_DIR}/worker_out.%h.%j.%t.%p --stdio_stderr ${RUN_DIR}/worker_error.%h.%j.%t.%p   --tasks_per_rs 1 --cpu_per_rs 1 --gpu_per_rs 1 --rs_per_host 6 --latency_priority gpu-cpu dask-worker --nthreads 1 --nprocs 1 --interface ib0 --no-dashboard --reconnect --scheduler-file $SCHEDULER_FILE &

# Now block until the desired number of workers is registered with the
# scheduler.  Yes, we're going to run this on the batch node because
# we don't want to use compute node resources for this one shot tiny script.
python3 /ccs/proj/YOURPROJ/path/to/wait_for_workers.py --verbose --target-workers $NUM_WORKERS --pause-time 15 $SCHEDULER_FILE

echo "About to run dask client"

# We don't need MPI since dask uses cloudpickle over TCP/IP.
# Also presumes your dask client python scripts accepts a
# --scheduler-file parameter to connect it to the above scheduler.
jsrun --smpiargs="none" --nrs 1  --tasks_per_rs 1 --cpu_per_rs 1  --gpu_per_rs 0 --rs_per_host 1 --latency_priority cpu-cpu python3 -u ${SRC_PATH}/path/to/your/dask/client/myclient.py --scheduler-file $SCHEDULER_FILE

echo "Finished running the dask client"

# We're done so kill the scheduler and worker processes
jskill all

echo Script is done.
